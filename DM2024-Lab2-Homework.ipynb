{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Student Information\n",
    "Name: ç›§å­æ¶µ\n",
    "\n",
    "Student ID: 113065542\n",
    "\n",
    "GitHub ID: luzi2023\n",
    "\n",
    "Kaggle name: luzi8451\n",
    "\n",
    "Kaggle private scoreboard snapshot: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. First: __This part is worth 30% of your grade.__ Do the **take home exercises** in the [DM2024-Lab2-master Repo](https://github.com/didiersalazar/DM2024-Lab2-Master). You may need to copy some cells from the Lab notebook to this notebook. \n",
    "\n",
    "\n",
    "2. Second: __This part is worth 30% of your grade.__ Participate in the in-class [Kaggle Competition](https://www.kaggle.com/competitions/dm-2024-isa-5810-lab-2-homework) regarding Emotion Recognition on Twitter by this link: https://www.kaggle.com/competitions/dm-2024-isa-5810-lab-2-homework. The scoring will be given according to your place in the Private Leaderboard ranking: \n",
    "    - **Bottom 40%**: Get 20% of the 30% available for this section.\n",
    "\n",
    "    - **Top 41% - 100%**: Get (0.6N + 1 - x) / (0.6N) * 10 + 20 points, where N is the total number of participants, and x is your rank. (ie. If there are 100 participants and you rank 3rd your score will be (0.6 * 100 + 1 - 3) / (0.6 * 100) * 10 + 20 = 29.67% out of 30%.)   \n",
    "    Submit your last submission **BEFORE the deadline (Nov. 26th, 11:59 pm, Tuesday)**. Make sure to take a screenshot of your position at the end of the competition and store it as '''pic0.png''' under the **img** folder of this repository and rerun the cell **Student Information**.\n",
    "    \n",
    "\n",
    "3. Third: __This part is worth 30% of your grade.__ A report of your work developing the model for the competition (You can use code and comment on it). This report should include what your preprocessing steps, the feature engineering steps and an explanation of your model. You can also mention different things you tried and insights you gained. \n",
    "\n",
    "\n",
    "4. Fourth: __This part is worth 10% of your grade.__ It's hard for us to follow if your code is messy :'(, so please **tidy up your notebook**.\n",
    "\n",
    "\n",
    "Upload your files to your repository then submit the link to it on the corresponding e-learn assignment.\n",
    "\n",
    "Make sure to commit and save your changes to your repository __BEFORE the deadline (Nov. 26th, 11:59 pm, Tuesday)__. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mç„¡æ³•å•Ÿå‹• Kernel 'myenv (Python 3.11.9)'ã€‚\n",
      "\u001b[1;31må¦‚éœ€è©³ç´°è³‡æ–™ï¼Œè«‹æª¢è¦– Jupyter <a href='command:jupyter.viewOutput'>è¨˜éŒ„</a>ã€‚ ENOSPC: no space left on device, write"
     ]
    }
   ],
   "source": [
    "### Begin Assignment Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### =======================Second Part================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import emoji\n",
    "import re\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "import transformers\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_tweet = \"tweets_DM.json\"\n",
    "data = []\n",
    "with open(raw_tweet, \"r\") as f_in:\n",
    "    for line in f_in:\n",
    "        data.append(json.loads(line))\n",
    "\n",
    "data_identification = pd.read_csv(\"./data_identification.csv\")\n",
    "submission = pd.read_csv(\"./sampleSubmission.csv\")\n",
    "# submission.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®šç¾©è¡¨æƒ…ç¬¦è™Ÿåˆ°é—œéµå­—çš„æ˜ å°„å­—å…¸\n",
    "emoji_dict = {\n",
    "    'ğŸ˜‚': '[joy]',\n",
    "    'â¤ï¸': '[love]',\n",
    "    'ğŸ˜': '[adoration]',\n",
    "    'ğŸ˜­': '[cry]',\n",
    "    'â¤': '[care]',\n",
    "    'ğŸ˜Š': '[happy]',\n",
    "    'ğŸ™': '[pray]',\n",
    "    'ğŸ˜˜': '[kiss]',\n",
    "    'ğŸ’•': '[love_each_other]',\n",
    "    'ğŸ”¥': '[fire]',\n",
    "    'ğŸ˜©': '[weary]',\n",
    "    'ğŸ¤”': '[think]',\n",
    "    'ğŸ’¯': '[perfect]',\n",
    "    'ğŸ’™': '[loyalty]',\n",
    "    'ğŸ™„': '[annoyed]',\n",
    "    'ğŸ˜': '[happy]',\n",
    "    'ğŸ™Œ': '[celebrate]',\n",
    "    'ğŸ™ğŸ¾': '[pray]',\n",
    "    'ğŸ‘': '[approve]',\n",
    "    'ğŸ™ğŸ½': '[pray]'\n",
    "}\n",
    "\n",
    "# emoji_dict = {\n",
    "#     'ğŸ˜‚': '[emoji]',\n",
    "#     'â¤ï¸': '[emoji]',\n",
    "#     'ğŸ˜': '[emoji]',\n",
    "#     'ğŸ˜­': '[emoji]',\n",
    "#     'â¤': '[emoji]',\n",
    "#     'ğŸ˜Š': '[emoji]',\n",
    "#     'ğŸ™': '[emoji]',\n",
    "#     'ğŸ˜˜': '[emoji]',\n",
    "#     'ğŸ’•': '[emoji]',\n",
    "#     'ğŸ”¥': '[emoji]',\n",
    "#     'ğŸ˜©': '[emoji]',\n",
    "#     'ğŸ¤”': '[emoji]',\n",
    "#     'ğŸ’¯': '[emoji]',\n",
    "#     'ğŸ’™': '[emoji]',\n",
    "#     'ğŸ™„': '[emoji]',\n",
    "#     'ğŸ˜': '[emoji]',\n",
    "#     'ğŸ™Œ': '[emoji]',\n",
    "#     'ğŸ™ğŸ¾': '[emoji]',\n",
    "#     'ğŸ‘': '[emoji]',\n",
    "#     'ğŸ™ğŸ½': '[emoji]'\n",
    "# }\n",
    "\n",
    "# å®šç¾©æ¸…ç†æ¨æ–‡æ–‡æœ¬çš„å‡½æ•¸\n",
    "def clean_tweet(text, emoji_dict):\n",
    "    # å°‡å®šç¾©çš„è¡¨æƒ…ç¬¦è™Ÿæ›¿æ›ç‚ºå°æ‡‰çš„é—œéµè©\n",
    "    for emj, keyword in emoji_dict.items():\n",
    "        text = text.replace(emj, keyword)\n",
    "    # ç§»é™¤å…¶é¤˜çš„è¡¨æƒ…ç¬¦è™Ÿ\n",
    "    text = emoji.replace_emoji(text, replace='')\n",
    "    # ç§»é™¤ <LH> æ¨™ç±¤\n",
    "    text = re.sub(r'<LH>', '', text)\n",
    "    # ç§»é™¤å¤šé¤˜çš„ç©ºç™½å­—å…ƒ\n",
    "    text = text.strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processed_tweets = []\n",
    "# for entry in data:\n",
    "#     # æª¢æŸ¥ '_source' å’Œ 'tweet' æ˜¯å¦å­˜åœ¨æ–¼è¨˜éŒ„ä¸­\n",
    "#     if '_source' in entry and 'tweet' in entry['_source']:\n",
    "#         tweet = entry['_source']['tweet']\n",
    "#         # æª¢æŸ¥ 'text' æ˜¯å¦å­˜åœ¨æ–¼ 'tweet' ä¸­\n",
    "#         if 'text' in tweet:\n",
    "#             tweet_text = tweet['text']\n",
    "#             cleaned_text = clean_tweet(tweet_text, emoji_dict)\n",
    "#             # å‰µå»ºè™•ç†å¾Œçš„æ¨æ–‡è¨˜éŒ„ï¼Œä¿ç•™ '_source' å’Œ 'tweet'\n",
    "#             processed_tweet = {\n",
    "#                 '_source': {\n",
    "#                     'tweet': tweet.copy()\n",
    "#                 }\n",
    "#             }\n",
    "#             # æ›´æ–°æ¸…ç†å¾Œçš„æ–‡æœ¬\n",
    "#             processed_tweet['_source']['tweet']['text'] = cleaned_text\n",
    "#             processed_tweets.append(processed_tweet)\n",
    "#         else:\n",
    "#             print(\"è¨˜éŒ„ä¸­ç¼ºå°‘ 'text' éµ\")\n",
    "#     else:\n",
    "#         print(\"è¨˜éŒ„ä¸­ç¼ºå°‘ '_source' æˆ– 'tweet' éµ\")\n",
    "\n",
    "# # å°‡è™•ç†å¾Œçš„æ¨æ–‡è³‡æ–™å­˜å„²ç‚º JSON æª”æ¡ˆ\n",
    "# with open('tweets_DM_filtered_1.json', 'w', encoding='utf-8') as outfile:\n",
    "#     json.dump(processed_tweets, outfile, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_tweets = []\n",
    "with open('tweets_DM_filtered_emoji.json', 'r', encoding='utf-8') as f_in:\n",
    "  data = json.load(f_in)\n",
    "  processed_tweets.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å°‡è™•ç†å¾Œçš„è³‡æ–™è½‰æ›ç‚º DataFrame\n",
    "df_processed = pd.DataFrame(processed_tweets)\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "_source = df['_source'].apply(lambda x: x['tweet'])\n",
    "df = pd.DataFrame({\n",
    "    'tweet_id': _source.apply(lambda x: x['tweet_id']),\n",
    "    'hashtags': _source.apply(lambda x: x['hashtags']),\n",
    "    'text': _source.apply(lambda x: x['text']),\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.merge(data_identification, on='tweet_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "emotion\n",
       "joy             516017\n",
       "anticipation    248935\n",
       "trust           205478\n",
       "sadness         193437\n",
       "disgust         139101\n",
       "fear             63999\n",
       "surprise         48729\n",
       "anger            39867\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotions = pd.read_csv(\"emotion.csv\")\n",
    "emotions['emotion'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1816555/2092790352.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_data.rename(columns={'tweet_id': 'id', 'hashtags': 'hashtags', 'text':'text', 'identification': 'identification'}, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "train_data = df[df['identification'] == 'train']\n",
    "test_data = df[df['identification'] == 'test']\n",
    "train_data = train_data.merge(emotions, on='tweet_id', how='left')\n",
    "train_data.drop_duplicates(subset=['text'], keep=False, inplace=True)\n",
    "\n",
    "# çµ±ä¸€submissionè·Ÿtest_dataçš„é †åº\n",
    "test_data.rename(columns={'tweet_id': 'id', 'hashtags': 'hashtags', 'text':'text', 'identification': 'identification'}, inplace=True)\n",
    "\n",
    "new_submission = submission.copy()\n",
    "\n",
    "new_submission = new_submission.merge(test_data, on='id')\n",
    "new_submission.drop(['hashtags', 'identification'], axis='columns', inplace=True)\n",
    "\n",
    "y_train_data = train_data['emotion']\n",
    "X_train_data = train_data['text']\n",
    "X_test_data = new_submission['text']\n",
    "\n",
    "# æ¨™ç±¤ç·¨ç¢¼\n",
    "le = preprocessing.LabelEncoder()\n",
    "y_train = le.fit_transform(y_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_submission.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_train_data, y_train, test_size=0.3, random_state=42, stratify=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel, DistilBertTokenizer\n",
    "\n",
    "# tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "# tokenizer = BertTokenizer.from_pretrained(\"huawei-noah/TinyBERT_General_4L_312D\")\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "max_length = 50\n",
    "tokenized_train = tokenizer(X_train.tolist(), max_length=max_length, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "# tokenized_val = tokenizer(X_val.tolist(), max_length=max_length, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "tokenized_test = tokenizer(X_test_data.tolist(), max_length=max_length, truncation=True, padding=True, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenized_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from torch import nn\n",
    "# from transformers import BertModel, BertTokenizer\n",
    "\n",
    "# class BertClassifier(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(BertClassifier, self).__init__()\n",
    "#         self.bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "#         # self.bert = BertModel.from_pretrained(\"huawei-noah/TinyBERT_General_4L_312D\")\n",
    "#         self.classifier = nn.Linear(self.bert.config.hidden_size, 8)\n",
    "\n",
    "#     def forward(self, input_ids, attention_mask, token_type_ids=None):\n",
    "#         # BERT æ¨¡å‹è¼¸å‡º\n",
    "#         outputs = self.bert(input_ids=input_ids, \n",
    "#                             attention_mask=attention_mask, \n",
    "#                             token_type_ids=token_type_ids\n",
    "#                             )\n",
    "#         pooled_output = outputs.pooler_output  # BERT çš„ [CLS] token è¡¨ç¤º\n",
    "#         logits = self.classifier(pooled_output)  # ç·šæ€§å±¤é€²è¡Œåˆ†é¡\n",
    "#         return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from transformers import DistilBertModel\n",
    "\n",
    "class BertClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BertClassifier, self).__init__()\n",
    "        # ä½¿ç”¨ DistilBERT æ¨¡å‹\n",
    "        self.bert = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, 8)  # å‡è¨­æœ‰ 8 å€‹åˆ†é¡\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # DistilBERT è¼¸å‡º last_hidden_state\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        # ä½¿ç”¨ last_hidden_state çš„ç¬¬ä¸€å€‹ token ä½œç‚ºå¥å­çš„è¡¨ç¤º\n",
    "        cls_output = outputs.last_hidden_state[:, 0, :]  # å– [CLS] token åµŒå…¥\n",
    "        logits = self.classifier(cls_output)  # ç·šæ€§å±¤é€²è¡Œåˆ†é¡\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = BertClassifier().to(device)\n",
    "\n",
    "input_ids = torch.tensor(tokenized_train['input_ids'].numpy()).to(device)\n",
    "attention_mask = torch.tensor(tokenized_train['attention_mask'].numpy()).to(device)\n",
    "# token_type_ids = torch.tensor(tokenized_train['token_type_ids'].numpy()).to(device)\n",
    "\n",
    "train_input = {\n",
    "    'input_ids': input_ids,\n",
    "    'attention_mask': attention_mask,\n",
    "    # 'token_type_ids': token_type_ids,\n",
    "}\n",
    "\n",
    "y_train = torch.tensor(y_train).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 62979/62979 [36:18<00:00, 28.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "Loss: 1.0975, Accuracy: 0.6032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 62979/62979 [36:30<00:00, 28.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5\n",
      "Loss: 0.9498, Accuracy: 0.6572\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 62979/62979 [36:30<00:00, 28.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5\n",
      "Loss: 0.8409, Accuracy: 0.6970\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 62979/62979 [36:30<00:00, 28.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5\n",
      "Loss: 0.7326, Accuracy: 0.7361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 62979/62979 [36:33<00:00, 28.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5\n",
      "Loss: 0.6289, Accuracy: 0.7739\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "batch_size = 16\n",
    "dataset = torch.utils.data.TensorDataset(\n",
    "    train_input['input_ids'], train_input['attention_mask'], y_train\n",
    ")\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    for batch in tqdm(dataloader):\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # å‰å‘å‚³æ’­\n",
    "        logits = model(input_ids, attention_mask)\n",
    "        \n",
    "        # è¨ˆç®—æå¤±\n",
    "        loss = criterion(logits, labels)\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # è¨ˆç®—æº–ç¢ºç‡\n",
    "        _, preds = torch.max(logits, dim=1)\n",
    "        correct_predictions += (preds == labels).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    "        \n",
    "        # åå‘å‚³æ’­å’Œæ›´æ–°æ¬Šé‡\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # æ¯å€‹ epoch çµæŸå¾Œï¼Œè¨ˆç®—å¹³å‡æå¤±å’Œæº–ç¢ºç‡\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = correct_predictions / total_samples\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "    print(f\"Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    if epoch == 3:\n",
    "        torch.save(model.state_dict(), \"distilbert_classifier_model_epoch_3.pth\")\n",
    "\n",
    "# ä¿å­˜æ¨¡å‹\n",
    "torch.save(model.state_dict(), \"distilbert_classifier_model_epoch_5.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"distilbert_classifier_model_epoch_3.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = TensorDataset(tokenized_test['input_ids'], tokenized_test['attention_mask'])\n",
    "batch_size = 16\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_ids = torch.tensor(tokenized_val['input_ids'].numpy()).to(device)\n",
    "# attention_mask = torch.tensor(tokenized_val['attention_mask'].numpy()).to(device)\n",
    "# token_type_ids = torch.tensor(tokenized_val['token_type_ids'].numpy()).to(device)\n",
    "test_ids = torch.tensor(tokenized_test['input_ids'].numpy()).to(device)\n",
    "test_attention_mask = torch.tensor(tokenized_test['attention_mask'].numpy()).to(device)\n",
    "\n",
    "# val_input = {\n",
    "#     'input_ids': input_ids,\n",
    "#     'attention_mask': attention_mask,\n",
    "#     # 'token_type_ids': token_type_ids,\n",
    "# }\n",
    "\n",
    "test_input = {\n",
    "    'input_ids': test_ids,\n",
    "    'attention_mask': test_attention_mask\n",
    "}\n",
    "\n",
    "# y_val = torch.tensor(y_val).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|â–ˆâ–ˆâ–ˆâ–‹      | 9631/25749 [01:26<02:24, 111.85it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m probabilities \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msoftmax(logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      8\u001b[0m predictions \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(probabilities, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m----> 9\u001b[0m class_labels \u001b[38;5;241m=\u001b[39m le\u001b[38;5;241m.\u001b[39minverse_transform(\u001b[43mpredictions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     10\u001b[0m pred\u001b[38;5;241m.\u001b[39mextend(class_labels)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31måœ¨ç›®å‰å„²å­˜æ ¼æˆ–ä¸Šä¸€å€‹å„²å­˜æ ¼ä¸­åŸ·è¡Œç¨‹å¼ç¢¼æ™‚ï¼ŒKernel å·²ææ¯€ã€‚\n",
      "\u001b[1;31mè«‹æª¢é–±å„²å­˜æ ¼ä¸­çš„ç¨‹å¼ç¢¼ï¼Œæ‰¾å‡ºå¤±æ•—çš„å¯èƒ½åŸå› ã€‚\n",
      "\u001b[1;31må¦‚éœ€è©³ç´°è³‡è¨Šï¼Œè«‹æŒ‰ä¸€ä¸‹<a href='https://aka.ms/vscodeJupyterKernelCrash'>é€™è£¡</a>ã€‚\n",
      "\u001b[1;31må¦‚éœ€è©³ç´°è³‡æ–™ï¼Œè«‹æª¢è¦– Jupyter <a href='command:jupyter.viewOutput'>è¨˜éŒ„</a>ã€‚"
     ]
    }
   ],
   "source": [
    "pred = []\n",
    "model.eval()  # è¨­ç½®æ¨¡å‹ç‚ºè©•ä¼°æ¨¡å¼\n",
    "with torch.no_grad():  # æ¨ç†æ™‚ä¸è¨ˆç®—æ¢¯åº¦\n",
    "    for batch in tqdm(test_loader):\n",
    "        input_ids, attention_mask = [x.to(device) for x in batch]\n",
    "        logits = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        probabilities = torch.softmax(logits, dim=-1)\n",
    "        predictions = torch.argmax(probabilities, dim=-1)\n",
    "        class_labels = le.inverse_transform(predictions.cpu())\n",
    "        pred.extend(class_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['emotion'] = pred\n",
    "submission.to_csv(\"submission_distilbert_epoch_5.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
