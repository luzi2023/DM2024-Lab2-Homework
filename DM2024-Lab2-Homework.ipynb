{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Student Information\n",
    "Name: 盧子涵\n",
    "\n",
    "Student ID: 113065542\n",
    "\n",
    "GitHub ID: luzi2023\n",
    "\n",
    "Kaggle name: luzi8451\n",
    "\n",
    "Kaggle private scoreboard snapshot: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. First: __This part is worth 30% of your grade.__ Do the **take home exercises** in the [DM2024-Lab2-master Repo](https://github.com/didiersalazar/DM2024-Lab2-Master). You may need to copy some cells from the Lab notebook to this notebook. \n",
    "\n",
    "\n",
    "2. Second: __This part is worth 30% of your grade.__ Participate in the in-class [Kaggle Competition](https://www.kaggle.com/competitions/dm-2024-isa-5810-lab-2-homework) regarding Emotion Recognition on Twitter by this link: https://www.kaggle.com/competitions/dm-2024-isa-5810-lab-2-homework. The scoring will be given according to your place in the Private Leaderboard ranking: \n",
    "    - **Bottom 40%**: Get 20% of the 30% available for this section.\n",
    "\n",
    "    - **Top 41% - 100%**: Get (0.6N + 1 - x) / (0.6N) * 10 + 20 points, where N is the total number of participants, and x is your rank. (ie. If there are 100 participants and you rank 3rd your score will be (0.6 * 100 + 1 - 3) / (0.6 * 100) * 10 + 20 = 29.67% out of 30%.)   \n",
    "    Submit your last submission **BEFORE the deadline (Nov. 26th, 11:59 pm, Tuesday)**. Make sure to take a screenshot of your position at the end of the competition and store it as '''pic0.png''' under the **img** folder of this repository and rerun the cell **Student Information**.\n",
    "    \n",
    "\n",
    "3. Third: __This part is worth 30% of your grade.__ A report of your work developing the model for the competition (You can use code and comment on it). This report should include what your preprocessing steps, the feature engineering steps and an explanation of your model. You can also mention different things you tried and insights you gained. \n",
    "\n",
    "\n",
    "4. Fourth: __This part is worth 10% of your grade.__ It's hard for us to follow if your code is messy :'(, so please **tidy up your notebook**.\n",
    "\n",
    "\n",
    "Upload your files to your repository then submit the link to it on the corresponding e-learn assignment.\n",
    "\n",
    "Make sure to commit and save your changes to your repository __BEFORE the deadline (Nov. 26th, 11:59 pm, Tuesday)__. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m無法啟動 Kernel 'myenv (Python 3.11.9)'。\n",
      "\u001b[1;31m如需詳細資料，請檢視 Jupyter <a href='command:jupyter.viewOutput'>記錄</a>。 ENOSPC: no space left on device, write"
     ]
    }
   ],
   "source": [
    "### Begin Assignment Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### =======================Second Part================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import emoji\n",
    "import re\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "import transformers\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_tweet = \"tweets_DM.json\"\n",
    "data = []\n",
    "with open(raw_tweet, \"r\") as f_in:\n",
    "    for line in f_in:\n",
    "        data.append(json.loads(line))\n",
    "\n",
    "data_identification = pd.read_csv(\"./data_identification.csv\")\n",
    "submission = pd.read_csv(\"./sampleSubmission.csv\")\n",
    "# submission.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定義表情符號到關鍵字的映射字典\n",
    "emoji_dict = {\n",
    "    '😂': '[joy]',\n",
    "    '❤️': '[love]',\n",
    "    '😍': '[adoration]',\n",
    "    '😭': '[cry]',\n",
    "    '❤': '[care]',\n",
    "    '😊': '[happy]',\n",
    "    '🙏': '[pray]',\n",
    "    '😘': '[kiss]',\n",
    "    '💕': '[love_each_other]',\n",
    "    '🔥': '[fire]',\n",
    "    '😩': '[weary]',\n",
    "    '🤔': '[think]',\n",
    "    '💯': '[perfect]',\n",
    "    '💙': '[loyalty]',\n",
    "    '🙄': '[annoyed]',\n",
    "    '😁': '[happy]',\n",
    "    '🙌': '[celebrate]',\n",
    "    '🙏🏾': '[pray]',\n",
    "    '👍': '[approve]',\n",
    "    '🙏🏽': '[pray]'\n",
    "}\n",
    "\n",
    "# emoji_dict = {\n",
    "#     '😂': '[emoji]',\n",
    "#     '❤️': '[emoji]',\n",
    "#     '😍': '[emoji]',\n",
    "#     '😭': '[emoji]',\n",
    "#     '❤': '[emoji]',\n",
    "#     '😊': '[emoji]',\n",
    "#     '🙏': '[emoji]',\n",
    "#     '😘': '[emoji]',\n",
    "#     '💕': '[emoji]',\n",
    "#     '🔥': '[emoji]',\n",
    "#     '😩': '[emoji]',\n",
    "#     '🤔': '[emoji]',\n",
    "#     '💯': '[emoji]',\n",
    "#     '💙': '[emoji]',\n",
    "#     '🙄': '[emoji]',\n",
    "#     '😁': '[emoji]',\n",
    "#     '🙌': '[emoji]',\n",
    "#     '🙏🏾': '[emoji]',\n",
    "#     '👍': '[emoji]',\n",
    "#     '🙏🏽': '[emoji]'\n",
    "# }\n",
    "\n",
    "# 定義清理推文文本的函數\n",
    "def clean_tweet(text, emoji_dict):\n",
    "    # 將定義的表情符號替換為對應的關鍵詞\n",
    "    for emj, keyword in emoji_dict.items():\n",
    "        text = text.replace(emj, keyword)\n",
    "    # 移除其餘的表情符號\n",
    "    text = emoji.replace_emoji(text, replace='')\n",
    "    # 移除 <LH> 標籤\n",
    "    text = re.sub(r'<LH>', '', text)\n",
    "    # 移除多餘的空白字元\n",
    "    text = text.strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processed_tweets = []\n",
    "# for entry in data:\n",
    "#     # 檢查 '_source' 和 'tweet' 是否存在於記錄中\n",
    "#     if '_source' in entry and 'tweet' in entry['_source']:\n",
    "#         tweet = entry['_source']['tweet']\n",
    "#         # 檢查 'text' 是否存在於 'tweet' 中\n",
    "#         if 'text' in tweet:\n",
    "#             tweet_text = tweet['text']\n",
    "#             cleaned_text = clean_tweet(tweet_text, emoji_dict)\n",
    "#             # 創建處理後的推文記錄，保留 '_source' 和 'tweet'\n",
    "#             processed_tweet = {\n",
    "#                 '_source': {\n",
    "#                     'tweet': tweet.copy()\n",
    "#                 }\n",
    "#             }\n",
    "#             # 更新清理後的文本\n",
    "#             processed_tweet['_source']['tweet']['text'] = cleaned_text\n",
    "#             processed_tweets.append(processed_tweet)\n",
    "#         else:\n",
    "#             print(\"記錄中缺少 'text' 鍵\")\n",
    "#     else:\n",
    "#         print(\"記錄中缺少 '_source' 或 'tweet' 鍵\")\n",
    "\n",
    "# # 將處理後的推文資料存儲為 JSON 檔案\n",
    "# with open('tweets_DM_filtered_1.json', 'w', encoding='utf-8') as outfile:\n",
    "#     json.dump(processed_tweets, outfile, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_tweets = []\n",
    "with open('tweets_DM_filtered_emoji.json', 'r', encoding='utf-8') as f_in:\n",
    "  data = json.load(f_in)\n",
    "  processed_tweets.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 將處理後的資料轉換為 DataFrame\n",
    "df_processed = pd.DataFrame(processed_tweets)\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "_source = df['_source'].apply(lambda x: x['tweet'])\n",
    "df = pd.DataFrame({\n",
    "    'tweet_id': _source.apply(lambda x: x['tweet_id']),\n",
    "    'hashtags': _source.apply(lambda x: x['hashtags']),\n",
    "    'text': _source.apply(lambda x: x['text']),\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.merge(data_identification, on='tweet_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "emotion\n",
       "joy             516017\n",
       "anticipation    248935\n",
       "trust           205478\n",
       "sadness         193437\n",
       "disgust         139101\n",
       "fear             63999\n",
       "surprise         48729\n",
       "anger            39867\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotions = pd.read_csv(\"emotion.csv\")\n",
    "emotions['emotion'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1816555/2092790352.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_data.rename(columns={'tweet_id': 'id', 'hashtags': 'hashtags', 'text':'text', 'identification': 'identification'}, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "train_data = df[df['identification'] == 'train']\n",
    "test_data = df[df['identification'] == 'test']\n",
    "train_data = train_data.merge(emotions, on='tweet_id', how='left')\n",
    "train_data.drop_duplicates(subset=['text'], keep=False, inplace=True)\n",
    "\n",
    "# 統一submission跟test_data的順序\n",
    "test_data.rename(columns={'tweet_id': 'id', 'hashtags': 'hashtags', 'text':'text', 'identification': 'identification'}, inplace=True)\n",
    "\n",
    "new_submission = submission.copy()\n",
    "\n",
    "new_submission = new_submission.merge(test_data, on='id')\n",
    "new_submission.drop(['hashtags', 'identification'], axis='columns', inplace=True)\n",
    "\n",
    "y_train_data = train_data['emotion']\n",
    "X_train_data = train_data['text']\n",
    "X_test_data = new_submission['text']\n",
    "\n",
    "# 標籤編碼\n",
    "le = preprocessing.LabelEncoder()\n",
    "y_train = le.fit_transform(y_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_submission.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_train_data, y_train, test_size=0.3, random_state=42, stratify=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel, DistilBertTokenizer\n",
    "\n",
    "# tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "# tokenizer = BertTokenizer.from_pretrained(\"huawei-noah/TinyBERT_General_4L_312D\")\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "max_length = 50\n",
    "tokenized_train = tokenizer(X_train.tolist(), max_length=max_length, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "# tokenized_val = tokenizer(X_val.tolist(), max_length=max_length, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "tokenized_test = tokenizer(X_test_data.tolist(), max_length=max_length, truncation=True, padding=True, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenized_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from torch import nn\n",
    "# from transformers import BertModel, BertTokenizer\n",
    "\n",
    "# class BertClassifier(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(BertClassifier, self).__init__()\n",
    "#         self.bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "#         # self.bert = BertModel.from_pretrained(\"huawei-noah/TinyBERT_General_4L_312D\")\n",
    "#         self.classifier = nn.Linear(self.bert.config.hidden_size, 8)\n",
    "\n",
    "#     def forward(self, input_ids, attention_mask, token_type_ids=None):\n",
    "#         # BERT 模型輸出\n",
    "#         outputs = self.bert(input_ids=input_ids, \n",
    "#                             attention_mask=attention_mask, \n",
    "#                             token_type_ids=token_type_ids\n",
    "#                             )\n",
    "#         pooled_output = outputs.pooler_output  # BERT 的 [CLS] token 表示\n",
    "#         logits = self.classifier(pooled_output)  # 線性層進行分類\n",
    "#         return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from transformers import DistilBertModel\n",
    "\n",
    "class BertClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BertClassifier, self).__init__()\n",
    "        # 使用 DistilBERT 模型\n",
    "        self.bert = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, 8)  # 假設有 8 個分類\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # DistilBERT 輸出 last_hidden_state\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        # 使用 last_hidden_state 的第一個 token 作為句子的表示\n",
    "        cls_output = outputs.last_hidden_state[:, 0, :]  # 取 [CLS] token 嵌入\n",
    "        logits = self.classifier(cls_output)  # 線性層進行分類\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = BertClassifier().to(device)\n",
    "\n",
    "input_ids = torch.tensor(tokenized_train['input_ids'].numpy()).to(device)\n",
    "attention_mask = torch.tensor(tokenized_train['attention_mask'].numpy()).to(device)\n",
    "# token_type_ids = torch.tensor(tokenized_train['token_type_ids'].numpy()).to(device)\n",
    "\n",
    "train_input = {\n",
    "    'input_ids': input_ids,\n",
    "    'attention_mask': attention_mask,\n",
    "    # 'token_type_ids': token_type_ids,\n",
    "}\n",
    "\n",
    "y_train = torch.tensor(y_train).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 62979/62979 [36:18<00:00, 28.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "Loss: 1.0975, Accuracy: 0.6032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 62979/62979 [36:30<00:00, 28.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5\n",
      "Loss: 0.9498, Accuracy: 0.6572\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 62979/62979 [36:30<00:00, 28.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5\n",
      "Loss: 0.8409, Accuracy: 0.6970\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 62979/62979 [36:30<00:00, 28.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5\n",
      "Loss: 0.7326, Accuracy: 0.7361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 62979/62979 [36:33<00:00, 28.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5\n",
      "Loss: 0.6289, Accuracy: 0.7739\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "batch_size = 16\n",
    "dataset = torch.utils.data.TensorDataset(\n",
    "    train_input['input_ids'], train_input['attention_mask'], y_train\n",
    ")\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    for batch in tqdm(dataloader):\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # 前向傳播\n",
    "        logits = model(input_ids, attention_mask)\n",
    "        \n",
    "        # 計算損失\n",
    "        loss = criterion(logits, labels)\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # 計算準確率\n",
    "        _, preds = torch.max(logits, dim=1)\n",
    "        correct_predictions += (preds == labels).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    "        \n",
    "        # 反向傳播和更新權重\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # 每個 epoch 結束後，計算平均損失和準確率\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = correct_predictions / total_samples\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "    print(f\"Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    if epoch == 3:\n",
    "        torch.save(model.state_dict(), \"distilbert_classifier_model_epoch_3.pth\")\n",
    "\n",
    "# 保存模型\n",
    "torch.save(model.state_dict(), \"distilbert_classifier_model_epoch_5.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"distilbert_classifier_model_epoch_3.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = TensorDataset(tokenized_test['input_ids'], tokenized_test['attention_mask'])\n",
    "batch_size = 16\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_ids = torch.tensor(tokenized_val['input_ids'].numpy()).to(device)\n",
    "# attention_mask = torch.tensor(tokenized_val['attention_mask'].numpy()).to(device)\n",
    "# token_type_ids = torch.tensor(tokenized_val['token_type_ids'].numpy()).to(device)\n",
    "test_ids = torch.tensor(tokenized_test['input_ids'].numpy()).to(device)\n",
    "test_attention_mask = torch.tensor(tokenized_test['attention_mask'].numpy()).to(device)\n",
    "\n",
    "# val_input = {\n",
    "#     'input_ids': input_ids,\n",
    "#     'attention_mask': attention_mask,\n",
    "#     # 'token_type_ids': token_type_ids,\n",
    "# }\n",
    "\n",
    "test_input = {\n",
    "    'input_ids': test_ids,\n",
    "    'attention_mask': test_attention_mask\n",
    "}\n",
    "\n",
    "# y_val = torch.tensor(y_val).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 9631/25749 [01:26<02:24, 111.85it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m probabilities \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msoftmax(logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      8\u001b[0m predictions \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(probabilities, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m----> 9\u001b[0m class_labels \u001b[38;5;241m=\u001b[39m le\u001b[38;5;241m.\u001b[39minverse_transform(\u001b[43mpredictions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     10\u001b[0m pred\u001b[38;5;241m.\u001b[39mextend(class_labels)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m在目前儲存格或上一個儲存格中執行程式碼時，Kernel 已損毀。\n",
      "\u001b[1;31m請檢閱儲存格中的程式碼，找出失敗的可能原因。\n",
      "\u001b[1;31m如需詳細資訊，請按一下<a href='https://aka.ms/vscodeJupyterKernelCrash'>這裡</a>。\n",
      "\u001b[1;31m如需詳細資料，請檢視 Jupyter <a href='command:jupyter.viewOutput'>記錄</a>。"
     ]
    }
   ],
   "source": [
    "pred = []\n",
    "model.eval()  # 設置模型為評估模式\n",
    "with torch.no_grad():  # 推理時不計算梯度\n",
    "    for batch in tqdm(test_loader):\n",
    "        input_ids, attention_mask = [x.to(device) for x in batch]\n",
    "        logits = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        probabilities = torch.softmax(logits, dim=-1)\n",
    "        predictions = torch.argmax(probabilities, dim=-1)\n",
    "        class_labels = le.inverse_transform(predictions.cpu())\n",
    "        pred.extend(class_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['emotion'] = pred\n",
    "submission.to_csv(\"submission_distilbert_epoch_5.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
